# ðŸ›¡ï¸ SOC Security Data Pipeline & Dashboard

> **CYNA â€“ Data Engineer Internship Technical Test**


## Overview

This project simulates a Security Operations Center (SOC) data pipeline.

The goal is to:

- collect security logs,

- enrich them using threat intelligence,

- and visualize the results in a dashboard that helps SOC analysts understand what is happening in the network.

The focus of this project is clarity, correctness, and realistic design choices, rather than building an overly complex system.

---

##  Architecture
---
<img width="2316" height="1091" alt="image" src="https://github.com/user-attachments/assets/c1ba9eb9-091e-465c-bde1-fb4eedce5b30" />

---
## Architectural Choices 

The main goal of this project was to build something **realistic, simple, and easy to understand**, similar to how a basic SOC pipeline works in practice.  
The focus was on **clarity and correctness**, rather than adding unnecessary complexity.

---

### Batch Processing Instead of Real-Time Streaming

This project uses **batch processing** instead of real-time streaming.

The reason is simple: the goal is to demonstrate **log ingestion, enrichment, and analysis**, not real-time alerting. In many SOC environments, logs are processed in batches for investigation, reporting, and enrichment with threat intelligence.

With batch processing:

- The pipeline runs when the backend is executed  
- The dashboard displays the latest processed data  
- The system remains stable and easy to debug  

This approach also works well with limited resources (4 CPU cores, 8 GB RAM).

---

### Python for Ingestion and Processing

Python is used for log ingestion, parsing, and enrichment.

Python was chosen because it makes it easy to:

- Parse raw log files  
- Clean and normalize data  
- Run SQL queries using DuckDB  
- Keep the code readable and maintainable  

Python is commonly used in both data engineering and security teams, making it a practical and natural choice.

---

### DuckDB as the Central Database

DuckDB is used as the main database for this project.

DuckDB fits this use case well because:

- It is fast for analytical queries  
- It runs locally without requiring a database server  
- It supports efficient SQL joins and aggregations  

This allows raw logs, threat intelligence, and enriched data to be stored and queried in one place without adding unnecessary infrastructure.

---

### SQL-Based Enrichment with Threat Intelligence

Malicious detection is implemented using SQL-based enrichment.

A SQL `LEFT JOIN` is performed between:

- Parsed IDS logs  
- The IPSUM threat intelligence feed  

An event is marked as malicious if the **source or destination IP** matches an IP in the threat intelligence dataset.

This approach is:

- Simple  
- Transparent  
- Easy to explain and audit  

It closely reflects how many real SOC systems identify malicious activity using known threat intelligence.
---
##  Running the Project
````md
````
1. Create a virtual environment
````
python -m venv .venv
source .venv/bin/activate   # macOS / Linux
.venv\Scripts\activate      # Windows
````

#### 2. Install dependencies

```bash
pip install -r requirements.txt
```

#### 3. Run the data pipeline

```bash
python main.py
```

#### 4. Start the dashboard

```bash
streamlit run dashboards/app.py
```
Open in browser:
```bash
http://localhost:8501
```
---
## Note: 
In the local both backend and steamlit is running inside the (.venv) .

## Steamlit Frontend
-  cd cyna-data-engineering-test 
-  python -m venv .venv
- .venv\Scripts\activate
- streamlit run dashboards/app.py

---
## Option 1: Run Backend Pipeline in Local
- cd cyna-data-engineering-test
- **Check whether its inside (.venv) or not, if it is the run.**
- python main.py 
 
##  Option 2: Run Backend Pipeline with Docker
- cd cyna-data-engineering-test
- docker compose up --build  **(No need to run inside .venv)**
 

---

### What This Project Does
The system performs three core security data tasks:

- Ingest security logs generated by an IDS log generator

- Ingest threat intelligence data (malicious IP list)

- Enrich and analyze the data, then display insights in a dashboard
---

## Tech Stack

- **Language:**   Python  
- **Database:**   DuckDB  
- **Data Processing:**   Pandas, SQL  
- **Threat Intelligence:** IPSUM  
- **Dashboard:** Streamlit, Plotly  
- **Log Generation:** Security Log Generator  
- **Containerization:** Docker  

---

## Data Pipeline

### Log Ingestion
- Parse raw IDS logs
- Normalize timestamps
- Store structured data in `raw_logs`

### Threat Intelligence Ingestion
- Load IPSUM feed
- Store data in `threat_ips`

### Enrichment
- Match `src_ip` / `dst_ip` against threat intelligence
- Flag malicious events
- Attach confidence score

Final dataset: `enriched_logs`

---

## SOC Dashboard

Designed for **real SOC usage**, not just visualization.

### Key Metrics
- Total events  
- Malicious events  
- Benign events  

### Visualizations
- Severity distribution  
- Protocol usage  
- Event timeline  
- Top destination assets  
- Malicious IP analysis  

### Filters
- Date range  
- Severity  
- Protocol  

---


## What Was Achieved

#### 1. What Works

- IDS logs are successfully ingested and parsed

- Threat intelligence data is loaded correctly

- Logs are enriched using IP reputation matching

- Enriched data is stored in an analytical database

- A SOC-style dashboard displays meaningful security insights

- The project runs both locally and via Docker

---

## What Insights the Dashboard Provides

The dashboard allows a SOC analyst to:

- Monitor the overall volume of security events

- Identify potentially malicious traffic

- See which destination systems are most targeted

- Analyze severity and protocol distributions

- Investigate malicious events in detail when they appear

- Filter events by date, severity, and protocol
---
## Challenges Faced

Malicious Event Detection :
- One challenge I faced was correctly identifying malicious events during the enrichment step.
After manually adding a known malicious IP to the ids.log file, the total number of events increased as expected, but the dashboard still showed zero malicious events. Since ingestion and storage were working correctly, the issue was not immediately obvious.
After investigation, I found that the problem was in the SQL join logic used to enrich logs with threat intelligence. The query did not correctly handle cases where either the source IP or destination IP matched a malicious IP.
I fixed this by updating the enrichment query to check both src_ip and dst_ip against the threat intelligence table and correctly propagate the confidence score. After this change, malicious events were correctly flagged and appeared in the dashboard.

Data Freshness Between Pipeline and Dashboard :
- Another challenge was ensuring the dashboard always reflected the latest data after the backend pipeline was re-run. At times, the dashboard continued to display old values because it was reading from an existing DuckDB file while Streamlit does not automatically re-trigger ingestion.
This was resolved by clearly separating responsibilities: the backend pipeline handles ingestion and enrichment, while the dashboard remains read-only. I also removed unnecessary caching to ensure the dashboard always reads the current database state after each pipeline run.

Batch Processing vs Real-Time Streaming :
- One challenge in this project is that the system works in batch mode rather than real time. Logs are generated and stored in files, and they are only processed when the backend pipeline is executed.Because of this, the dashboard does not update automatically when new logs are added unless the ingestion step is run again.
I chose this approach intentionally because it is simpler, more stable, and easier to reason about within the scope of this technical test. With more time and infrastructure, this could be improved by using real-time streaming tools like Kafka, but for this project, batch processing was the most realistic and reliable choice.

Designing a Realistic SOC Dashboard :
- Making the dashboard look professional and SOC like was also challenging. Streamlit dashboards can easily appear demo-like if not designed carefully.
To address this, I focused on clarity and realism by using neutral colors, limiting red to threat-related signals, and structuring the dashboard into clear sections for overview, prioritization, and investigation. This resulted in a dashboard that is readable, calm, and aligned with how SOC analysts typically consume security data.


## What is not done yet

- No Real-Time Streaming (Batch Processing Only)

- No Automated Log Generator Integration

- No Multi-Source Log Support (IDS Logs Only)

---
---
## Dashboard Behavior (Malicious Events)
<img width="1074" height="238" alt="image" src="https://github.com/user-attachments/assets/15f074a6-23eb-4dda-ab01-3776a6f82d57" />

At the moment, the dashboard may show 0 Malicious Events.
This is expected behavior and does not indicate a problem.

This happens when none of the IP addresses in ids.log match the IPs present in the IPSUM threat intelligence dataset.

In this system, an event is marked as malicious only if the source IP or destination IP exists in the IPSUM dataset. If there is no match, the event is treated as benign.

To confirm that the detection and enrichment logic works correctly, I tested the pipeline by manually adding a known malicious IP from the IPSUM dataset into the IDS logs.

## Example Test Log
```bash
2026-02-06 12:00:00,000 - ids_logger_1 - high_severity - TCP - 213.209.159.158:4444 --> 10.0.0.10:80 - SYN - Malicious traffic

```
The IP address 213.209.159.158 exists in the IPSUM dataset with a confidence level of 11.

After rerunning the backend pipeline:

- The event was correctly flagged as malicious

- The Malicious Events count increased

- Malicious IPs, the heatmap, and event details appeared in the dashboard

